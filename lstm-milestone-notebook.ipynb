{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import Dense, Embedding, LSTM, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(filename):\n",
    "  data = pd.read_csv(filename)\n",
    "  return data['sequence'].values + data['q8'].values\n",
    "\n",
    "def get_output(filename):\n",
    "  output = []\n",
    "  file = np.load(filename)\n",
    "  for key in file:\n",
    "    output.append(file[key])\n",
    "  return output\n",
    "\n",
    "def get_ngram_text(seqs, n=8):\n",
    "    return np.array([[seq[i:i+n] for i in range(len(seq))] for seq in seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = get_input('train_input.csv')\n",
    "train_output = get_output('train_output.npz')\n",
    "test_input = get_input('test_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxlen_seq = max([len(seq) for seq in train_input])\n",
    "maxlen_seq = 128\n",
    "\n",
    "input_grams = get_ngram_text(train_input)\n",
    "\n",
    "tokenizer_encoder = Tokenizer()\n",
    "tokenizer_encoder.fit_on_texts(input_grams)\n",
    "input_data = tokenizer_encoder.texts_to_sequences(input_grams)\n",
    "input_data = sequence.pad_sequences(input_data, maxlen=maxlen_seq, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = []\n",
    "for output in train_output:\n",
    "    output_data.append(np.average(output))\n",
    "\n",
    "output_data = np.array(output_data)\n",
    "\n",
    "n_words = len(tokenizer_encoder.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaime/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 124251392 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3643 samples, validate on 911 samples\n",
      "Epoch 1/20\n",
      " - 189s - loss: 215.6754 - val_loss: 93.7914\n",
      "Epoch 2/20\n",
      " - 183s - loss: 66.7312 - val_loss: 42.2645\n",
      "Epoch 3/20\n",
      " - 183s - loss: 39.1871 - val_loss: 32.3220\n",
      "Epoch 4/20\n",
      " - 1667s - loss: 34.4569 - val_loss: 31.2690\n",
      "Epoch 5/20\n",
      " - 182s - loss: 33.9721 - val_loss: 31.2487\n",
      "Epoch 6/20\n",
      " - 182s - loss: 33.9346 - val_loss: 31.2755\n",
      "Epoch 7/20\n",
      " - 182s - loss: 33.9327 - val_loss: 31.2994\n",
      "Epoch 8/20\n",
      " - 183s - loss: 33.9234 - val_loss: 31.2812\n",
      "Epoch 9/20\n",
      " - 182s - loss: 33.9285 - val_loss: 31.2587\n",
      "Epoch 10/20\n",
      " - 182s - loss: 33.9288 - val_loss: 31.2562\n",
      "Epoch 11/20\n",
      " - 182s - loss: 33.9307 - val_loss: 31.2545\n",
      "Epoch 12/20\n",
      " - 181s - loss: 33.9224 - val_loss: 31.2938\n",
      "Epoch 13/20\n",
      " - 182s - loss: 33.9373 - val_loss: 31.3075\n",
      "Epoch 14/20\n",
      " - 181s - loss: 33.9237 - val_loss: 31.2710\n",
      "Epoch 15/20\n",
      " - 182s - loss: 33.9257 - val_loss: 31.2457\n",
      "Epoch 16/20\n",
      " - 194s - loss: 33.9569 - val_loss: 31.2627\n",
      "Epoch 17/20\n",
      " - 185s - loss: 33.9348 - val_loss: 31.2598\n",
      "Epoch 18/20\n",
      " - 182s - loss: 33.9319 - val_loss: 31.3011\n",
      "Epoch 19/20\n",
      " - 182s - loss: 33.9363 - val_loss: 31.3217\n",
      "Epoch 20/20\n",
      " - 182s - loss: 33.9390 - val_loss: 31.3211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb91b62e80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_text = Input(shape=(None, ))\n",
    "embedding_layer = Embedding(input_dim=n_words, output_dim=128)(sequence_text)\n",
    "x = LSTM(128)(embedding_layer)\n",
    "x = Activation('relu')(x)\n",
    "y = Dense(1, activation='relu')(x)\n",
    "\n",
    "model = Model(sequence_text, y)\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=n_words, output_dim=100))\n",
    "# model.add(LSTM(100))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# input_data = input_data.reshape((len(input_data), 1, 1382))\n",
    "\n",
    "model.fit(input_data, output_data,\n",
    "        epochs=20, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_grams = get_ngram_text(test_input)\n",
    "output_data = tokenizer_encoder.texts_to_sequences(test_input_grams)\n",
    "output_data = sequence.pad_sequences(output_data, maxlen=maxlen_seq, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_matrix = []\n",
    "test_input = pd.read_csv('test_input.csv')\n",
    "\n",
    "for index,row in test_input.iterrows():\n",
    "    \n",
    "    # For each test example, create a temp_matrix having all values same as predicted value\n",
    "    temp_matrix = np.full((row['length'],row['length']),predictions[index][0])\n",
    "    \n",
    "    # Set the diagonal values to 0\n",
    "    np.fill_diagonal(temp_matrix, 0)\n",
    "    \n",
    "    # Append to final_matrix\n",
    "    final_matrix.append(temp_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('test_%d.npz'%5,*final_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
